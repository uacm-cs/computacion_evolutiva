{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representación del texto: Word Embeddings\n",
    "\n",
    "- ### **Word embeddings** son representaciones numéricas densas y continuas de palabras en un espacio vectorial.\n",
    "- ### Estas representaciones capturan relaciones semánticas y sintácticas entre palabras.\n",
    "- ### Palabras con significados similares están más cercanas en el espacio vectorial.\n",
    "- ### Densidad: Cada palabra se representa como un vector en un espacio de dimensiones reducidas (por ejemplo, 100 o 300 dimensiones).\n",
    "- ### Diferente a las representaciones como las matrices dispersas en el modelo de \"bolsa de palabras\".\n",
    "- ### Similitud semántica: Las palabras con significados similares tendrán vectores cercanos en el espacio vectorial.\n",
    "- ### Por ejemplo, en un buen modelo de embeddings, los vectores de \"rey\" y \"reina\" estarán cerca.\n",
    "- ### Relaciones semánticas y aritmética vectorial:\n",
    "- ### Se pueden realizar operaciones matemáticas que reflejan relaciones semánticas, como:\n",
    "- ### **rey−hombre+mujer≈reina**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enfoques Word Embeddings:\n",
    "- #### los embeddings generalmente se entrenan a partir de grandes cantidades de texto utilizando algoritmos que buscan capturar las co-ocurrencias de palabras en un contexto dado. \n",
    "- #### Algunos métodos populares son:\n",
    "- #### **Word2Vec**:\n",
    "   - #### Utiliza dos enfoques: Skip-Gram (predice el contexto dada una palabra) y CBOW (predice una palabra dado su contexto).\n",
    "- #### **GloVe** (Global Vectors for Word Representation):\n",
    "    - #### Basado en una matriz de co-ocurrencia de palabras en un corpus grande.\n",
    "    - #### Intenta capturar la probabilidad relativa de dos palabras que co-ocurren.\n",
    "- #### **FastText**:\n",
    "    - #### Similar a Word2Vec, pero considera subpalabras (caracteres), lo que mejora la representación de palabras raras o con errores ortográficos.\n",
    "- #### **Contextuales (p. ej., BERT, GPT)**:\n",
    "    - #### Modelos que generan representaciones de palabras dependiendo del contexto en el que aparecen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos FastText\n",
    "\n",
    "### [https://fasttext.cc](https://fasttext.cc)\n",
    "\n",
    "## Modelo pre-entrenados para el idioma español\n",
    "\n",
    "### [https://fasttext.cc/docs/en/crawl-vectors.html](https://fasttext.cc/docs/en/crawl-vectors.html#models)\n",
    "\n",
    "\n",
    "## Modelo pre-entrenados para diferentes regiones del idioma español\n",
    "\n",
    "### [https://ingeotec.github.io/regional-spanish-models](https://ingeotec.github.io/regional-spanish-models/#resources)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalación del paquete FastText. \n",
    "### Se recomienda la instalación en sistemas operativos linux o mac os.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar el modelo pre-entrenado para la codificación de word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "# Descargar el modelo para el español de la página de FastText\n",
    "ft = fasttext.load_model('/Volumes/data/temp/cc.es.300.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtener el vector de una palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Obtención del vector de una palabra de palabras\n",
    "print(ft.get_word_vector(\"hola\"))\n",
    "\n",
    "# equivalente \n",
    "# Vector Denso de la palabra \"hola\"\n",
    "print(ft[\"hola\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total de palabras en el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtiene la lista total de palabras del modelo\n",
    "# ft.get_words()\n",
    "\n",
    "# Equivalente a la propiedad words\n",
    "\n",
    "# Obtención el total del vocabulario\n",
    "print(\"total de palabras: \", len(ft.words))\n",
    "\n",
    "#primeras 10 palabras del vocabulario\n",
    "ft.words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificar oraciones en su forma de embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtiene la representación de embedding de la oración\n",
    "\n",
    "vec = ft.get_sentence_vector(\"hola me siento muy feliz\")\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtención de las palabras vecinas más cercanas basadas en vectores densos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft.get_nearest_neighbors(\"mareado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operaciones con vectores semánticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= (ft.get_word_vector(\"rey\") - ft.get_word_vector(\"hombre\")) + ft.get_word_vector(\"mujer\")\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operaciones con vectores semánticos: Analogías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogia = ft.get_analogies(\"rey\",\"hombre\", \"mujer\")\n",
    "print(analogia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesando archivos que ya contiene su representación en word enbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print(f\"Cargando datos...\")\n",
    "dataset_train = pd.read_json(\"./data/dataset_polaridad_es_train_embeddings.json\", lines=True)\n",
    "dataset_test = pd.read_json(\"./data/dataset_polaridad_es_test_embeddings.json\", lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datos del train\n",
    "dataset_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datos del test\n",
    "dataset_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenando al clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "LEncoder = LabelEncoder()\n",
    "\n",
    "# El texto ya está en su forma vectorial calculado por los word embeddings del archivo según el campo\n",
    "# Campo = Vector de Word Embeddings de 300 dimensiones\n",
    "# we_ft = Word Embeddings calculados de textos generales del español \n",
    "# we_mx = Word Embeddings calculados de textos del español de México\n",
    "# we_es = Word Embeddings calculados de textos del español de España\n",
    "\n",
    "X_train_text = dataset_train['text'].to_numpy()\n",
    "# Convertir a matriz de arrays de numpy\n",
    "X_train = np.vstack(dataset_train[\"we_ft\"].to_numpy())\n",
    "Y_train = dataset_train['klass'].to_numpy()\n",
    "\n",
    "X_test_text = dataset_test['text'].to_numpy()\n",
    "# Convertir a matriz de arrays de numpy\n",
    "X_test = np.vstack(dataset_test[\"we_ft\"].to_numpy())\n",
    "Y_test = dataset_test['klass'].to_numpy()\n",
    "\n",
    "Y_train_encoded= LEncoder.fit_transform(Y_train)\n",
    "Y_test_encoded= LEncoder.transform(Y_test)\n",
    "\n",
    "\n",
    "# clf = svm.LinearSVC() = svm.SVC(kernel='linear')  son equivalentes los dos modelos.  \n",
    "# kernel{‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’}\n",
    "clf = svm.SVC(kernel='linear')\n",
    "# clf = svm.SVC(kernel='rbf')\n",
    "\n",
    "# Entrenar el modelo con los word embeddings\n",
    "clf.fit(X_train, Y_train_encoded)\n",
    "# Predecir los datos con el modelo entrenado para el conjunto de test\n",
    "y_pred = clf.predict(X_test)\n",
    "# Evaluar el desempeño de acuerdo a la métrica f1-macro\n",
    "score = f1_score(Y_test_encoded, y_pred, average=\"macro\")\n",
    "print(f\"Desempeño del modelo en el conjunto de test: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Ejercicio 1**\n",
    "### Entrenar y evaluar el desempeño del clasificador entrenado en el conjunto de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Probar con diferentes kernels del clasificador Support Vector Machine (SVM)\n",
    "- ### lineal, rbf, sigmoid, poly.\n",
    "### 2. Probar diferentes tipos de word embeddings previamente codificados:\n",
    "- ### we_ft (General) , we_mx (México), we_es (España) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ejercicio 2**\n",
    "- ### Modificar el algoritmo genético definido en el archivo $07\\_GA\\_clasificacion\\_texto\\_SVM.py$ para implementar el fitness con el desempeño del clasificador usando representación de word embeddings y el mejor kernel identificado para la predicción de datos con word embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
