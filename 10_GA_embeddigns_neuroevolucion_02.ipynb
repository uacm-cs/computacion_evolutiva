{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuroevolución y Word Embeddings\n",
    "- ### La neuroevolución es una técnica de inteligencia artificial que combina redes neuronales con algoritmos evolutivos.\n",
    "- ### Optimiza automáticamente la estructura y parámetros de una red.\n",
    "\n",
    "## Inspiración biológica:\n",
    "\n",
    "- ### Se basa en la evolución natural (selección, mutación y recombinación).\n",
    "\n",
    "- ### Las redes neuronales \"evolucionan\" para adaptarse a una tarea.\n",
    "\n",
    "## Tipos principales:\n",
    "\n",
    "- ### Evolución de pesos: Optimiza los parámetros: pesos y sesgos ($bias$) de una red fija.\n",
    "\n",
    "- ### Evolución de topologías: Modifica la arquitectura de la red (Por ejemplo, el método NEAT).\n",
    "\n",
    "## Ventajas:\n",
    "\n",
    "- ### No requiere del cálculo de gradientes (útil para problemas no diferenciables).\n",
    "\n",
    "- ### Explora múltiples soluciones en paralelo.\n",
    "\n",
    "- ### Puede descubrir arquitecturas novedosas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representación del texto: Word Embeddings\n",
    "\n",
    "- ### **Word embeddings** son representaciones numéricas densas y continuas de palabras en un espacio vectorial.\n",
    "- ### Estas representaciones capturan relaciones semánticas y sintácticas entre palabras.\n",
    "- ### Palabras con significados similares están más cercanas en el espacio vectorial.\n",
    "- ### Densidad: Cada palabra se representa como un vector en un espacio de dimensiones reducidas (por ejemplo, 100 o 300 dimensiones).\n",
    "- ### Diferente a las representaciones como las matrices dispersas en el modelo de \"bolsa de palabras\".\n",
    "- ### Similitud semántica: Las palabras con significados similares tendrán vectores cercanos en el espacio vectorial.\n",
    "- ### Por ejemplo, en un buen modelo de embeddings, los vectores de \"rey\" y \"reina\" estarán cerca.\n",
    "- ### Relaciones semánticas y aritmética vectorial:\n",
    "- ### Se pueden realizar operaciones matemáticas que reflejan relaciones semánticas, como:\n",
    "- ### **rey−hombre+mujer≈reina**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación de Textos por medio de Neuroevolución y Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figs/fig-diagrama-clasificador2.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenar al clasificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasificador: Red Neuronal Multicapa\n",
    "- #### Define una red con una arquitectura que consta de:\n",
    "    - #### 2 datos de entrada ($x_1$, $x_2$)\n",
    "    - #### capa 1 (4 neuronas)\n",
    "    - #### capa 2 (3 neuronas)\n",
    "    - #### capa 3 (2 neuronas): 2 datos de salida ($y_1$, $y_2$)\n",
    "\n",
    "<center>\n",
    "<img src=\"figs/fig-red_neuronal.png\" width=\"800\" style=\"background-color:white;\">\n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "- #### Número de parámetros de la red:\n",
    "    - #### Pesos en la capa 1: $w_{ij}^{(1)}$ = 8 (2 entradas x 4 neuronas) y   4 sesgos ($bias$) (1 de cada neurona)\n",
    "    - #### Pesos en la capa 2: $w_{ij}^{(2)}$ = 12 (4 entradas [4 neuronas de la capa 1] x 3 neuronas) y  3 sesgos ($bias$) (1 de cada neurona)\n",
    "    - #### Pesos en la capa 3: $w_{ij}^{(3)}$ = 6 (3 entradas [3 neuronas de la capa 2] x 2 neuronas) y  2 sesgos ($bias$) (1 de cada neurona)\n",
    "    - #### Total de parámetros: 35 (pesos y $bias$)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cargar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de ejemplos de entrenamiento\n",
      "klass\n",
      "neutral     1485\n",
      "positive     968\n",
      "negative     689\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Variables globales \n",
    "_NEURONAS = 128\n",
    "_TASA_REDUCCION = 0.2\n",
    "_N_CLASES = 3\n",
    "_TIPO_WORD_EMBEDDINGS = \"we_ft\"   # word embeddings creados de documentos del español general: posibles valores {\"we_ft\", \"we_mx\", \"we_es\"}\n",
    "\n",
    "\n",
    "dataset = pd.read_json(\"./data/dataset_polaridad_es_train_embeddings.json\", lines=True)\n",
    "#conteo de clases\n",
    "print(\"Total de ejemplos de entrenamiento\")\n",
    "print(dataset.klass.value_counts())\n",
    "# Extracción de los word embeddings y \n",
    "X = np.vstack(dataset[_TIPO_WORD_EMBEDDINGS].to_numpy())\n",
    "# Extracción de las etiquetas o clases de entrenamiento\n",
    "Y = dataset['klass'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Codificar las categorías (clases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases:\n",
      "['negative' 'neutral' 'positive']\n",
      "Clases codificadas:\n",
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "# Normalizar las etiquetas a una codificación ordinal para entrada del clasificador\n",
    "Y_encoded= le.fit_transform(Y)\n",
    "print(\"Clases:\")\n",
    "print(le.classes_)\n",
    "print(\"Clases codificadas:\")\n",
    "print(le.transform(le.classes_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preparar los conjuntos de datos  (datasets) para entrenamiento y para probar el rendimiento del clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [1]\n",
      " [0]\n",
      " ...\n",
      " [1]\n",
      " [2]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "# Dividir el conjunto de datos en conjunto de entrenamiento (80%) y conjunto de pruebas (20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, Y_train, Y_val =  train_test_split(X, Y_encoded, test_size=0.2, stratify=Y_encoded, random_state=42)\n",
    "\n",
    "Y_train = Y_train[:, np.newaxis] # Agregar una dimensión adicional para representar 1 ejemplo de entrenamiento por fila\n",
    "Y_val = Y_val[:, np.newaxis] # Agregar una dimensión adicional para representar 1 ejemplo de entrenamiento por fila\n",
    "\n",
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Definición de la arquitectura de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import numpy as np\n",
    "# Definir la red neuronal en PyTorch heredando de la clase base de Redes Neuronales: Module\n",
    "class RedNeuronal(nn.Module):\n",
    "    def __init__(self, tam_entrada, tam_capa_oculta, tam_salida, tasa_reduccion = 0.3):\n",
    "        super().__init__()\n",
    "        # Redondeado hacia arriba, reducir con la tasa de reducción \"reduction_rate\"\n",
    "        self.tam_capa_oculta2 = int(tam_capa_oculta - np.ceil(tam_capa_oculta * tasa_reduccion))\n",
    "        # print(f\"tamaño capa oculta2: {self.tam_capa_oculta2 }\")\n",
    "\n",
    "        # Definición de capas, funciones de activación e inicialización de pesos\n",
    "        # Capa Fully Connected (Capa Totalmente Conectada)\n",
    "        self.fc1 = nn.Linear(tam_entrada, tam_capa_oculta)\n",
    "        self.fc2 = nn.Linear(tam_capa_oculta, self.tam_capa_oculta2)\n",
    "        self.fc3 = nn.Linear(self.tam_capa_oculta2, tam_salida)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.tam_capa_1_pesos = tam_entrada * tam_capa_oculta \n",
    "        self.tam_capa_1_bias  =  tam_capa_oculta # bias \n",
    "        self.tam_capa_2_pesos = tam_capa_oculta * self.tam_capa_oculta2 \n",
    "        self.tam_capa_2_bias =  self.tam_capa_oculta2 # bias \n",
    "        self.tam_capa_3_pesos = self.tam_capa_oculta2 * tam_salida\n",
    "        self.tam_capa_3_bias =  tam_salida # bias \n",
    "        self.tam_individuo = int(self.tam_capa_1_pesos + self.tam_capa_1_bias  +  self.tam_capa_2_pesos + self.tam_capa_2_bias + self.tam_capa_3_pesos + self.tam_capa_3_bias)\n",
    "        # print(f\"tamaño individuo (genoma): {self.tam_individuo  }\")\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Definición del orden de conexión de las capas y aplición de las funciones de activación\n",
    "        out = self.fc1(X)\n",
    "        out = self.relu(out)  # Aplicamos la función de activación\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)  # Aplicamos la función de activación\n",
    "        out = self.fc3(out)\n",
    "        out = self.softmax(out)  # Aplicamos la activación softmax para obtenr la probabilidad de cada neurona de salida\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Definición del algoritmo evolutivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from multiprocessing import  cpu_count\n",
    "from sklearn.metrics import f1_score\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------\n",
    "# Funciones del algoritmo genético\n",
    "#------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------\n",
    "\n",
    "def inicializar_poblacion(tam_poblacion, tam_entrada, tam_capa_oculta, tam_salida, tasa_reduccion):\n",
    "    # Cada individuo es del tamaño del total de los pesos y bias que conforman a la red neuronal\n",
    "    # Un individuo está formado por los pesos y bias de todas sus capas en forma de un vector\n",
    "    # indviduo = [w11(1), w12(1), w13(1), b1(1), b2(1), w11(2), w12(2, b1(2), b2(2), ....]\n",
    "    # w11(1) representa el peso w11 de la capa 1\n",
    "    # wij(n) representa un peso i,j de la capa n\n",
    "    # bk(n) representa un sesgo (bias) k de la capa n\n",
    "    \n",
    "    red = RedNeuronal(tam_entrada, tam_capa_oculta, tam_salida, tasa_reduccion)    \n",
    "    tam_individuo = red.tam_individuo\n",
    "    poblacion = np.random.uniform(low=-0.5, high=0.5, size=(tam_poblacion, tam_individuo))\n",
    "    return poblacion\n",
    "\n",
    "#--------------------------------------------------------------------    \n",
    "def inicializar_poblacion_xavier(tam_poblacion, tam_entrada, tam_capa_oculta, tam_salida, tasa_reduccion):\n",
    "    poblacion = []\n",
    "    for k in range(tam_poblacion):        \n",
    "        red = RedNeuronal(tam_entrada, tam_capa_oculta, tam_salida, tasa_reduccion)\n",
    "        nn.init.xavier_normal_(red.fc1.weight)\n",
    "        nn.init.xavier_normal_(red.fc2.weight)\n",
    "        nn.init.xavier_normal_(red.fc3.weight)\n",
    "        nn.init.xavier_normal_(red.fc1.bias)\n",
    "        nn.init.xavier_normal_(red.fc2.bias)\n",
    "        nn.init.xavier_normal_(red.fc3.bias)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            individuo = np.concatenate((red.fc1.weight.flatten(), \n",
    "                        red.fc1.bias.flatten(),\n",
    "                        red.fc2.weight.flatten(), \n",
    "                        red.fc2.bias.flatten(),\n",
    "                        red.fc3.weight.flatten(), \n",
    "                        red.fc3.bias.flatten()))\n",
    "\n",
    "            poblacion.append(individuo)\n",
    "    poblacion = np.array(poblacion)        \n",
    "    print(f\"tamaño población: {poblacion.shape}\")\n",
    "    return poblacion\n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "def ajustar_estructura_red_neuronal(individuo, red_neuronal, tam_entrada, tam_capa_oculta, tam_salida):\n",
    " # Cargar pesos del individuo a la estrcutura de la red neuronal\n",
    "    with torch.no_grad():\n",
    "        # La matriz de pesos en Pytorch tienen la forma transpuesta (salida, entradas) a la inversa como se define la capa Lineal\n",
    "        # Se reestablece el vector de pesos y bias que representa a cada individuo a su estrcutura de la red neuronal\n",
    "        # 1. Se extraen las secciones (slice) del individuo (vector) que corresponden a la capa1 (fc1) y se extraen los bias de la capa1 (fc1)\n",
    "        # 1.2. Se asignan a las secciones correspondiente de la red (weight.data) y (bias.data). \n",
    "        # 1.3.  Deben tener la misma forma (shape) que la estructura de la red definida, de lo contrario indicará el error.\n",
    "        # 2. Se repiten el proceso para las capas restantes: desplazándose la sección de los pesos y bias de la primera capa (fc1) y extraer\n",
    "        #    los pesos y bias de la capa2 (fc2) y asignarlos a los parámetros correspondientes.\n",
    "        red_neuronal.fc1.weight.data = torch.tensor(individuo[:red_neuronal.tam_capa_1_pesos].reshape(tam_capa_oculta, tam_entrada)).float()\n",
    "        desplazamiento_capa_1 =   red_neuronal.tam_capa_1_pesos + red_neuronal.tam_capa_1_bias\n",
    "        red_neuronal.fc1.bias.data = torch.tensor(individuo[red_neuronal.tam_capa_1_pesos:desplazamiento_capa_1]).float()\n",
    "        \n",
    "        red_neuronal.fc2.weight.data = torch.tensor(individuo[desplazamiento_capa_1:desplazamiento_capa_1 + red_neuronal.tam_capa_2_pesos].reshape(red_neuronal.tam_capa_oculta2, tam_capa_oculta)).float()\n",
    "        desplazamiento_capa_2 = desplazamiento_capa_1 + red_neuronal.tam_capa_2_pesos + red_neuronal.tam_capa_2_bias\n",
    "        red_neuronal.fc2.bias.data = torch.tensor(individuo[desplazamiento_capa_1 + red_neuronal.tam_capa_2_pesos:desplazamiento_capa_2]).float()\n",
    "        \n",
    "        red_neuronal.fc3.weight.data = torch.tensor(individuo[desplazamiento_capa_2:desplazamiento_capa_2 + red_neuronal.tam_capa_3_pesos].reshape(tam_salida, red_neuronal.tam_capa_oculta2)).float()\n",
    "        desplazamiento_capa_3 = desplazamiento_capa_2 + red_neuronal.tam_capa_3_pesos + red_neuronal.tam_capa_3_bias\n",
    "        red_neuronal.fc3.bias.data = torch.tensor(individuo[desplazamiento_capa_2 + red_neuronal.tam_capa_3_pesos:desplazamiento_capa_3]).float()\n",
    "    return red_neuronal\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "#  Función para evaluar el individuo (que representa su genoma)\n",
    "def funcion_fitness(individuo, X, Y, tam_entrada, tam_capa_oculta, tam_salida, tasa_reduccion):\n",
    "    red_neuronal = RedNeuronal(tam_entrada, tam_capa_oculta, tam_salida, tasa_reduccion)\n",
    "    red_neuronal = ajustar_estructura_red_neuronal(individuo, red_neuronal, tam_entrada, tam_capa_oculta, tam_salida)\n",
    "\n",
    "    # Calcular precisión\n",
    "\n",
    "    X_tensor = torch.tensor(X).float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        y_pred = red_neuronal(X_tensor)\n",
    "        # Obtiene una única clase, la más probable\n",
    "        y_pred = torch.argmax(y_pred, dim=1)\n",
    "        score = f1_score(Y, y_pred, average=\"macro\")\n",
    "            \n",
    "    return score\n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "def evaluar_fitness(poblacion,  X, Y, tam_entrada, tam_capa_oculta, tam_salida, tasa_reduccion):\n",
    "    fitness = []\n",
    "    for individuo in poblacion:\n",
    "        val_fitness = funcion_fitness(individuo, X, Y, tam_entrada, tam_capa_oculta, tam_salida, tasa_reduccion)\n",
    "        fitness.append(val_fitness)\n",
    "    return  np.array(fitness)\n",
    "\n",
    "    \n",
    "#--------------------------------------------------------------------\n",
    "def evaluar_fitness_paralelo(poblacion,  X, Y, tam_entrada, tam_capa_oculta, tam_salida, tasa_reduccion, n_jobs=-1):\n",
    "    \"\"\"Evalúa la población en paralelo usando multiprocessing.\"\"\"\n",
    "    print(f\"Evaluando población en paralelo ({n_jobs} núcleos)...\")\n",
    "    # Configurar el número de workers (n_jobs=-1 usa todos los núcleos)\n",
    "    n_jobs = cpu_count() if n_jobs == -1 else n_jobs\n",
    "\n",
    "    # Evaluar en paralelo\n",
    "\n",
    "    resultados = Parallel(\n",
    "    n_jobs = n_jobs-1,\n",
    "    timeout = 300,  # 5min\n",
    "    verbose = 10\n",
    "    )(\n",
    "        delayed(funcion_fitness)(individuo, X, Y, tam_entrada, tam_capa_oculta, tam_salida, tasa_reduccion)\n",
    "        for individuo in poblacion\n",
    "    )\n",
    "\n",
    "    fitness = np.array(resultados)\n",
    "    \n",
    "    return fitness\n",
    "    \n",
    "#------------------------------------------------------------------------\n",
    "def seleccionar_padres(poblacion, aptitudes, k=5):\n",
    "    \"\"\"Selecciona dos padres mediante torneo.\"\"\"\n",
    "    torneo = random.sample(list(zip(poblacion, aptitudes)), k=k)\n",
    "    torneo.sort(key=lambda x: x[1])  \n",
    "    return torneo[0][0], torneo[1][0]\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "def elitismo(poblacion, fitness, tam_elite=2):\n",
    "    \"\"\"Selecciona los 'tam_elite' mejores best_individuos.\"\"\"\n",
    "    # Ordenar por fitness (mayor = mejor)\n",
    "    ranked_indices = np.argsort(fitness)[::-1]\n",
    "    elites = [poblacion[i] for i in ranked_indices[:tam_elite]]\n",
    "    return elites\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "def cruzar(padre1, padre2):\n",
    "    punto_cruza = np.random.randint(len(padre1))\n",
    "    hijo1 = np.concatenate([padre1[:punto_cruza], padre2[punto_cruza:]])\n",
    "    hijo2 = np.concatenate([padre2[:punto_cruza], padre1[punto_cruza:]])\n",
    "    return hijo1, hijo2\n",
    "\n",
    "def cruzar_uniforme(padre1, padre2):\n",
    "    #Cruza uniforme\n",
    "    probabilidad_intercambio=0.5\n",
    "    # Generar máscara de intercambio\n",
    "    mascara = np.random.random(len(padre1)) < probabilidad_intercambio\n",
    "    # Crear hijos\n",
    "    hijo1 = np.where(mascara, padre2, padre1)\n",
    "    hijo2 = np.where(mascara, padre1, padre2)\n",
    "    return hijo1, hijo2\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "def mutar(individuo, tasa_mutacion=0.1, sigma=1):\n",
    "    # Crea la mascara de genes por mutar que cumplan con la condición\n",
    "    mascara = np.random.rand(len(individuo)) < tasa_mutacion\n",
    "    individuo += mascara * np.random.normal(0, sigma, size=len(individuo))  # Mutar al individuo en los genes seleccionados \n",
    "    return individuo\n",
    "\n",
    "    \n",
    "def algoritmo_evolutivo(X, Y, tam_poblacion=30, num_generaciones=50, run_paralelo=False):\n",
    "    tam_entrada = X.shape[1]  # Total de Características: dimensiones del word embeddings\n",
    "    tam_capa_oculta = _NEURONAS\n",
    "    tam_salida = _N_CLASES\n",
    "    tasa_reduccion = _TASA_REDUCCION\n",
    "        \n",
    "    # Población inicial\n",
    "    poblacion = inicializar_poblacion(tam_poblacion, tam_entrada, tam_capa_oculta, tam_salida, tasa_reduccion)\n",
    "    # poblacion = inicializar_poblacion_xavier(tam_poblacion, tam_entrada, tam_capa_oculta, tam_salida, tasa_reduccion)\n",
    "    best_fitness_hist = []\n",
    "    mean_fitness_hist = []\n",
    "\n",
    "    for generacion in range(num_generaciones):\n",
    "        # Evaluar fitness (usando el conjunto de entrenamiento)\n",
    "        if run_paralelo:\n",
    "            val_fitness = evaluar_fitness_paralelo(poblacion,  X, Y, tam_entrada, tam_capa_oculta, tam_salida, tasa_reduccion, n_jobs=-1)\n",
    "        else:            \n",
    "            val_fitness = evaluar_fitness(poblacion, X, Y, tam_entrada, tam_capa_oculta, tam_salida, tasa_reduccion)\n",
    "        \n",
    "        # Registrar estadísticas\n",
    "        print(\"obteniendo mejor fitness\" )\n",
    "        best_fitness = np.max(val_fitness)\n",
    "        print(\"obteniendo media fitness \" )\n",
    "        mean_fitness = np.mean(val_fitness)\n",
    "        best_fitness_hist.append(best_fitness)\n",
    "        mean_fitness_hist.append(mean_fitness)\n",
    "        \n",
    "        print(f\"Generación {generacion + 1}: Mejor fitness = {best_fitness:.4f}, Fitness promedio = {mean_fitness:.4f}\")\n",
    "        nueva_poblacion = []\n",
    "\n",
    "        for _ in range(tam_poblacion // 2):\n",
    "            # Seleccionar padres\n",
    "            padre1, padre2 = seleccionar_padres(poblacion, val_fitness)            \n",
    "            hijo1, hijo2 = cruzar(padre1, padre2)\n",
    "            hijo1 = mutar(hijo1)\n",
    "            hijo2 = mutar(hijo2)\n",
    "            nueva_poblacion.append(hijo1)            \n",
    "            nueva_poblacion.append(hijo2)        \n",
    "        \n",
    "        \n",
    "        #-----------------\n",
    "        # Población: Los hijos sustituyen a los padres\n",
    "        #-----------------\n",
    "        # poblacion = np.array(nueva_poblacion)\n",
    "    \n",
    "        #-----------------\n",
    "        # Población con elitismo de padres y parte de los hijos\n",
    "        #-----------------\n",
    "        nueva_poblacion = np.array(nueva_poblacion)\n",
    "        K_best_padres = 10\n",
    "        poblacion[:K_best_padres, ] = elitismo(poblacion, val_fitness, K_best_padres)\n",
    "        poblacion[K_best_padres:, ] = nueva_poblacion[K_best_padres:, ]\n",
    "\n",
    "\n",
    "    if run_paralelo:\n",
    "        val_fitness = evaluar_fitness_paralelo(poblacion,  X, Y, tam_entrada, tam_capa_oculta, tam_salida, tasa_reduccion, n_jobs=-1)\n",
    "    else:            \n",
    "        val_fitness = evaluar_fitness(poblacion, X, Y, tam_entrada, tam_capa_oculta, tam_salida, tasa_reduccion)\n",
    "\n",
    "\n",
    "    # Evaluar el mejor modelo en train\n",
    "    best_individuo = poblacion[np.argmax(val_fitness)]\n",
    "    test_f1 = funcion_fitness(best_individuo, X, Y, tam_entrada, tam_capa_oculta, tam_salida, tasa_reduccion)\n",
    "    print(f\"\\nF1-score final en test: {test_f1:.3f}\")\n",
    "    return best_individuo\n",
    "\n",
    "\n",
    "def predecir_clase(individuo, X, tam_entrada, tam_capa_oculta, tam_salida, tasa_reduccion):\n",
    "    red_neuronal = RedNeuronal(tam_entrada, tam_capa_oculta, tam_salida, tasa_reduccion)\n",
    "    red_neuronal = ajustar_estructura_red_neuronal(individuo, red_neuronal, tam_entrada, tam_capa_oculta, tam_salida)\n",
    " \n",
    "    # Calcular precisión\n",
    "    X_tensor = torch.from_numpy(X).float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Calcula las predicciones con la red neuronal con los pesos y bias definidos por el individuo\n",
    "        y_pred = red_neuronal(X_tensor)\n",
    "        y_pred = torch.argmax(y_pred, dim=1)\n",
    "        return y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Ejecución del algoritmo evolutivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 1: Mejor fitness = 0.3023, Fitness promedio = 0.1906\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 2: Mejor fitness = 0.3251, Fitness promedio = 0.2151\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 3: Mejor fitness = 0.3730, Fitness promedio = 0.2133\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 4: Mejor fitness = 0.3730, Fitness promedio = 0.2116\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 5: Mejor fitness = 0.3730, Fitness promedio = 0.2166\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 6: Mejor fitness = 0.3730, Fitness promedio = 0.1994\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 7: Mejor fitness = 0.3730, Fitness promedio = 0.1943\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 8: Mejor fitness = 0.3730, Fitness promedio = 0.1985\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 9: Mejor fitness = 0.3730, Fitness promedio = 0.2004\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 10: Mejor fitness = 0.3730, Fitness promedio = 0.1913\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 11: Mejor fitness = 0.3730, Fitness promedio = 0.2058\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 12: Mejor fitness = 0.3730, Fitness promedio = 0.1910\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 13: Mejor fitness = 0.3730, Fitness promedio = 0.1794\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 14: Mejor fitness = 0.3730, Fitness promedio = 0.1777\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 15: Mejor fitness = 0.3730, Fitness promedio = 0.1736\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 16: Mejor fitness = 0.3730, Fitness promedio = 0.1816\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 17: Mejor fitness = 0.3730, Fitness promedio = 0.1733\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 18: Mejor fitness = 0.3730, Fitness promedio = 0.1838\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 19: Mejor fitness = 0.3730, Fitness promedio = 0.1764\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 20: Mejor fitness = 0.3730, Fitness promedio = 0.1763\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 21: Mejor fitness = 0.3730, Fitness promedio = 0.1761\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 22: Mejor fitness = 0.3730, Fitness promedio = 0.1707\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 23: Mejor fitness = 0.3730, Fitness promedio = 0.1763\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 24: Mejor fitness = 0.3730, Fitness promedio = 0.1822\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 25: Mejor fitness = 0.3730, Fitness promedio = 0.1800\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 26: Mejor fitness = 0.3730, Fitness promedio = 0.1744\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 27: Mejor fitness = 0.3730, Fitness promedio = 0.1794\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 28: Mejor fitness = 0.3730, Fitness promedio = 0.1769\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 29: Mejor fitness = 0.3730, Fitness promedio = 0.1744\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 30: Mejor fitness = 0.3730, Fitness promedio = 0.1710\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 31: Mejor fitness = 0.3730, Fitness promedio = 0.1725\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 32: Mejor fitness = 0.3730, Fitness promedio = 0.1743\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 33: Mejor fitness = 0.3730, Fitness promedio = 0.1714\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 34: Mejor fitness = 0.3730, Fitness promedio = 0.1708\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 35: Mejor fitness = 0.3730, Fitness promedio = 0.1738\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 36: Mejor fitness = 0.3730, Fitness promedio = 0.1718\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 37: Mejor fitness = 0.3730, Fitness promedio = 0.1708\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 38: Mejor fitness = 0.3730, Fitness promedio = 0.1713\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 39: Mejor fitness = 0.3730, Fitness promedio = 0.1707\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 40: Mejor fitness = 0.3730, Fitness promedio = 0.1705\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 41: Mejor fitness = 0.3730, Fitness promedio = 0.1708\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 42: Mejor fitness = 0.3730, Fitness promedio = 0.1705\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 43: Mejor fitness = 0.3730, Fitness promedio = 0.1706\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 44: Mejor fitness = 0.3730, Fitness promedio = 0.1706\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 45: Mejor fitness = 0.3730, Fitness promedio = 0.1717\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 46: Mejor fitness = 0.3730, Fitness promedio = 0.1705\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 47: Mejor fitness = 0.3730, Fitness promedio = 0.1705\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 48: Mejor fitness = 0.3730, Fitness promedio = 0.1707\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 49: Mejor fitness = 0.3730, Fitness promedio = 0.1714\n",
      "obteniendo mejor fitness\n",
      "obteniendo media fitness \n",
      "Generación 50: Mejor fitness = 0.3730, Fitness promedio = 0.1706\n",
      "\n",
      "F1-score final en test: 0.373\n"
     ]
    }
   ],
   "source": [
    "\n",
    "best_individuo = algoritmo_evolutivo(X_train, Y_train, tam_poblacion=50, num_generaciones=50, run_paralelo=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Prueba del mejor individuo en el conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de ejemplos de entrenamiento\n",
      "klass\n",
      "neutral     371\n",
      "positive    242\n",
      "negative    173\n",
      "Name: count, dtype: int64\n",
      "[[2]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n",
      "tensor([2, 0, 2, 0, 2])\n",
      "\n",
      "F1-score final en test: 0.388\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "tam_entrada = X_train.shape[1] # Tamaño del word embeddigns\n",
    "tam_capa_oculta = _NEURONAS\n",
    "tam_salida = _N_CLASES\n",
    "tasa_reduccion = _TASA_REDUCCION\n",
    "\n",
    "dataset_test = pd.read_json(\"./data/dataset_polaridad_es_test_embeddings.json\", lines=True)\n",
    "#conteo de clases\n",
    "print(\"Total de ejemplos de entrenamiento\")\n",
    "print(dataset_test.klass.value_counts())\n",
    "# Extracción de los textos en arreglos de numpy\n",
    "X_test = np.vstack(dataset_test[_TIPO_WORD_EMBEDDINGS].to_numpy())\n",
    "# Extracción de las etiquetas o clases de entrenamiento\n",
    "Y_test = dataset_test['klass'].to_numpy()\n",
    "\n",
    "Y_test = le.transform(Y_test)\n",
    "Y_t = Y_test[:, np.newaxis] # Agregar una dimensión adicional para representar 1 ejemplo de entrenamiento por fila\n",
    "y_pred_test = predecir_clase(best_individuo, X_test, tam_entrada, tam_capa_oculta, tam_salida, tasa_reduccion)\n",
    "print(Y_t[:5])\n",
    "print(y_pred_test[:5])\n",
    "score = f1_score(Y_t, y_pred_test, average=\"macro\")\n",
    "print(f\"\\nF1-score final en test: {score:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Probar diferentes tipos de word embeddings.\n",
    "- ### Campo del dataset \"we_ft\": Word Embeddings construidos con documentos del español general\n",
    "- ### Campo del dataset \"we_mx\": Word Embeddings construidos con tuits del español de México\n",
    "- ### Campo del dataset \"we_es\": Word Embeddings construidos con tuits del español de España\n",
    "### 2. Probar diferentes tipos de operadores de variación:\n",
    "- ### Cruza (1 punto, 2 puntos, n puntos)\n",
    "### 3. Variar el tamaño de la población (considerar que en estos casos el costo computacional es mayor)\n",
    "### 4. Variar el número de generaciones (considerar que en estos casos el costo computacional es mayor)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
